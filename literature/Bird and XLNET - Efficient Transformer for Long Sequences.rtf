{\rtf1\ansi\ansicpg1252\cocoartf2578
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Big Bird and XLNET - Efficient Transformer for Long Sequences\
\
Since the introduction of Transformer networks, many different approaches have been proposed to achieve better results on common tasks. However, many of the proposed methods require a huge amount of processing resources and training time and are often not trainable at all for large sequences. Lately published architectures like Big Bird [1] and XLNET [2]  introduce new methods to address this problem and reduce  parameters and training time to make these architectures suitable for long sequences. \
\
The goal of this work is to compare Big Bird and XLNET with traditional transformer networks with a detailed description of their key mechanics and what they do to require less resources and lower the computation time. \
\
[1] 
\f1 \cf2 \expnd0\expndtw0\kerning0
Zaheer, Manzil, et al. "Big bird: Transformers for longer sequences."\
[2] Yang, Zhilin, et al. "Xlnet: Generalized autoregressive pretraining for language understanding."}