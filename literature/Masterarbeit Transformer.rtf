{\rtf1\ansi\ansicpg1252\cocoartf2578
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Roman;\f1\froman\fcharset0 Times-Bold;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs24 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Masterarbeit/Projektseminar\
\
EEG based Mental State Prediction with Transformer Networks.\
\
Machine Learning\
\
\pard\pardeftab720\sa240\partightenfactor0
\cf2 4/5 3/5\
The analysis of neurobiological data can help to detect diseases at an early stage, to treat symptoms more effectively and to improve the quality of life for some patients. Since data acquisition is time-consuming and cost-intensive, only a limited amount of neurobiological data is available for training neuronal networks. This leads to overfitting during training with traditional time series analysis methods and to an insufficient generalization to unseen data. Lately published Transformer networks [1] have proven their capabilities to outperform traditional RNN approaches for a variety	of tasks and can overcome the limitations of smaller datasets [2, 3]. The goal of this Master thesis  is to evaluate different  Transformer networks like AlBERT [3], Big Bird and XLNET [5].\
[1] Vaswani, Ashish, et al. "Attention is all you need." \
[2] Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding."\
[3] Lan, Zhenzhong, et al. "Albert: A lite bert for self-supervised learning of language representations.\
[4] Zaheer, Manzil, et al. "Big bird: Transformers for longer sequences." \
[5] Yang, Zhilin, et al. "Xlnet: Generalized autoregressive pretraining for language understanding.\'94
\f1\b \
Requirements
\f0\b0 : Python, Tensorflow/Keras/PyTorch\
}